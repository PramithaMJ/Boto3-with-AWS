{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3742a6f4-80a5-491d-ac0b-8a142e41dab9",
   "metadata": {},
   "source": [
    "![https://pieriantraining.com/](../PTCenteredPurple.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738cd05-3d41-47b5-b29b-715fb73d13ec",
   "metadata": {},
   "source": [
    "## Capstone Project: AWS Media Library Management System\n",
    "\n",
    "In this capstone project, we'll delve into the integration of AWS services with Python through Boto3. Our objective is to construct a robust system for users to seamlessly upload media files such as images, videos, and audio, which will then be carefully processed, cataloged, and securely stored on AWS.\n",
    "\n",
    "This project offers the opportunity to apply your knowledge of AWS and Boto3 in a practical scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823b3b2-09b3-4ac7-ac8a-f29cfb09e684",
   "metadata": {},
   "source": [
    "### Components:\n",
    "\n",
    "1. S3 Bucket - To store the uploaded media files.\n",
    "2. DynamoDB - To store the metadata of each media file.\n",
    "3. Lambda Functions - To process media files after upload.\n",
    "4. Searching - To obtain stored results and download them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd784c9b-4ca8-4485-9524-8a581e1ccc12",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a4795-ac68-4485-99cb-6aee68b95dfd",
   "metadata": {},
   "source": [
    "### 1. Infrastructure\n",
    "1. Create an S3 bucket for storing media files.\n",
    "2. Set up a DynamoDB table to store the metadata of the media files. If you want, you can create a secondary key for the file type\n",
    "3. Configure / Verify your IAM roles and permissions needed for Lambda, S3, DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03783b40-020d-4cec-92b5-db309e21ba43",
   "metadata": {},
   "source": [
    "**Create S3 Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779c743-c25c-46ad-b9d5-3f44a60597d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create S3 Bucket ###\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9a6d7-ab45-437c-83d9-6f1eee9b306f",
   "metadata": {},
   "source": [
    "**Create Table Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7b244-9481-4035-9822-6683d9822fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic Table Definition ###\n",
    "table_name = # TODO\n",
    "attributes = [\n",
    "\n",
    "    # TODO. Feel free to use as many attributes as you want\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "key_schema = [\n",
    "    \n",
    "    # TODO\n",
    "]\n",
    "\n",
    "provisioned_throughput = {\n",
    "    'ReadCapacityUnits': 5,\n",
    "    'WriteCapacityUnits': 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab1481-e695-4cbb-b304-10b2aca9543e",
   "metadata": {},
   "source": [
    "**Create the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39bf34-e28b-4f5d-827f-183a63ce9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database\n",
    "\n",
    "response = dynamo_client.create_table(\n",
    "    #TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34d8bd-1c34-47eb-b320-f7279262e4c6",
   "metadata": {},
   "source": [
    "### 2. Media Upload\n",
    "\n",
    "1. Use Boto3 to create a Python function to upload media files to the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e930d9-62fe-43a4-b9af-5dd8c9d6b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def upload_to_s3(local_file_path, bucket_name):\n",
    "    \"\"\"\n",
    "    Uploads a file to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "    - local_file_path (str): Path to the local file to be uploaded.\n",
    "    - bucket_name (str): Name of the S3 bucket where the file should be uploaded.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the uploaded file in the S3 bucket in the format \"bucket_name/filename\".\n",
    "    \"\"\"\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0817498-404c-4946-bca8-fda16d49eb74",
   "metadata": {},
   "source": [
    "### 3. Processing Media\n",
    "1. Create a Lambda function that extracts the metadata from the media (file type, size) and saves it to the DynamoDB table.\n",
    "2. This function should be triggered, once a file is uploaded\n",
    "3. Save your code to a .py file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d47adf-e223-4859-91bf-13be64c24177",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lambda code. Similar to lambda.py. Do not run this code here, as it will raise an exception ###\n",
    "\n",
    "### Required lambda imports and clients\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "###\n",
    "\n",
    "### Extract metadata from the file ###\n",
    "def extract_metadata(event):\n",
    "    \"\"\"\n",
    "    Extracts metadata from an S3 event.\n",
    "\n",
    "    Args:\n",
    "    - event (dict): The S3 event from which metadata is to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - bucket (str): The name of the S3 bucket.\n",
    "        - key (str): The key (path) of the object in the S3 bucket.\n",
    "        - file_type (str): The file type (extension) of the object, or \"None\" if it doesn't have an extension.\n",
    "        - size (int): The size of the object in bytes.\n",
    "\n",
    "    Note:\n",
    "    Assumes the event is an S3 PutObject event and has the appropriate structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "\n",
    "### Add metadata to database. Use file identifier as id ###\n",
    "def add_to_database(bucket, key, file_type, size):\n",
    "    \"\"\"\n",
    "    Adds metadata information to a DynamoDB table.\n",
    "\n",
    "    Args:\n",
    "    - bucket (str): The name of the S3 bucket where the file is located.\n",
    "    - key (str): The key (path) of the object in the S3 bucket.\n",
    "    - file_type (str): The file type (extension) of the object.\n",
    "    - size (int): The size of the object in bytes.\n",
    "\n",
    "    Outputs:\n",
    "    - Prints the response from the DynamoDB put_item operation.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "\n",
    "### Lambda handler routine ###\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    AWS Lambda function handler that processes an S3 event, extracts metadata \n",
    "    from the event, and then adds the metadata to a DynamoDB table.\n",
    "\n",
    "    Args:\n",
    "    - event (dict): The S3 event triggered when a new object is added to the bucket.\n",
    "    - context (obj): AWS Lambda context object (not used, but included as it's a standard parameter).\n",
    "\n",
    "    Outputs:\n",
    "    - Prints the file type and size (in kilobytes) of the uploaded object.\n",
    "    - Calls the `add_to_database` function to store the metadata in DynamoDB.\n",
    "    - Prints the response from the DynamoDB operation within the `add_to_database` function.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4f629-8209-40f8-bc85-6db15fd40a35",
   "metadata": {},
   "source": [
    "**Save your lambda code as a .py file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a82a385-6748-4ae5-aed8-2721e969f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867dff33-9801-41d2-880f-c4fd9eca6afd",
   "metadata": {},
   "source": [
    "**Create an IAM role for Lambda trigger. Note that it needs to have s3 Read access as well as dynamodb PUT access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3772f4b-f35d-4020-9c4c-43ee5b4a75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IAM role for Lambda trigger. Note that it needs to have s3 Read access as well as dynamodb PUT access\n",
    "\n",
    "lambda_execution_policy = {\n",
    "    # TODO\n",
    "}\n",
    "\n",
    "role_response = iam_client.create_role(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "iam_client.put_role_policy(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "# Get the ARN of the created role\n",
    "role_arn = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba828d5d-0eb0-42e0-a7ad-d9ecb0dccea0",
   "metadata": {},
   "source": [
    "**Create the lambda function**<br />\n",
    "Note that you should read your function_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a957c268-3446-4793-abb0-711db82fa7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in function_code\n",
    "with open(# TODO)\n",
    "    function_code # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e160aa1-ff2b-417a-b1ad-a93de48141f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Lambda function ###\n",
    "function_name = # TODO\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "\n",
    "with io.BytesIO() as deployment_package:\n",
    "    with zipfile.ZipFile(deployment_package, 'w') as zipf:\n",
    "        zipf.writestr(# TODO)\n",
    "\n",
    "    create_function_response = lambda_client.create_function(\n",
    "       # TODO\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c765df-8c94-495e-8b22-e83b80d18b60",
   "metadata": {},
   "source": [
    "**Create Inline Permission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866e42e-9567-49a4-9dca-52dd113a8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inline Permission ###\n",
    "\n",
    "bucket_arn = # TODO\n",
    "lambda_client.add_permission(\n",
    "     # TODO\n",
    " )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6c132-8cad-4d41-b76b-ff116b3f05ad",
   "metadata": {},
   "source": [
    "**Define Event Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a319dd-f901-459f-82d5-ef95074879f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the event configuration ###\n",
    "event_configuration = {\n",
    "    # TODO\n",
    "}\n",
    "\n",
    "# Configure the S3 event trigger\n",
    "s3_client.put_bucket_notification_configuration(\n",
    "    # TODO\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21e8ab-19db-479c-ac23-b58dae1e1d45",
   "metadata": {},
   "source": [
    "### 4. Upload Data.\n",
    "Let's upload some data. You can use the data provided in the **data** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd0e25-484a-4435-9e23-11437b84cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in Path(\"data/\").glob(\"*\"):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb20f47-a197-4981-a1cc-bfdd8c58be8f",
   "metadata": {},
   "source": [
    "### 5. Search and Retrieve Media:\n",
    "\n",
    "1. Create a Python function using Boto3 that allows users to search the DynamoDB table based on various parameters (file type, size, etc ...).\n",
    "2. The function should return a list of files that match the search criteria.\n",
    "3. Allow users to download the file from the S3 bucket using a pre-signed URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ba585-ef50-455a-99c5-7edd30beaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dynamodb(file_type=None, size=0):\n",
    "    \"\"\"\n",
    "    Searches a DynamoDB table for media files based on their file type and/or size.\n",
    "\n",
    "    Args:\n",
    "    - file_type (str, optional): The file type (extension) to filter by. Defaults to None.\n",
    "    - size (int, optional): The minimum size (in kilobytes) to filter by. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of items (dicts) from the DynamoDB table that match the search criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a86b08-d0f9-438a-a65d-1937cb86ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_presigned_urls(query_response_list, expiration=300):\n",
    "    \"\"\"\n",
    "    Generates presigned URLs for items in a given DynamoDB query response list.\n",
    "\n",
    "    Args:\n",
    "    - query_response_list (list): A list of items (dicts) from a DynamoDB query response.\n",
    "    - expiration (int, optional): The number of seconds the presigned URL is valid for. Defaults to 300 seconds.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where:\n",
    "        - key: The 'id' (in the format \"bucket/key\") of the item from the query response.\n",
    "        - value: The generated presigned URL for the corresponding item.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
